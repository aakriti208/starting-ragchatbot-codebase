# Copy this file to .env and configure your LLM provider

# LLM Provider Selection: "anthropic", "ollama", or "localai"
LLM_PROVIDER=anthropic

# Anthropic Claude API Settings (for anthropic provider)
ANTHROPIC_API_KEY=your-anthropic-api-key-here
ANTHROPIC_MODEL=claude-sonnet-4-20250514

# Ollama Local LLM Settings (for ollama provider)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# LocalAI Settings (for localai provider)
LOCALAI_BASE_URL=http://localhost:8080
LOCALAI_MODEL=gpt-3.5-turbo